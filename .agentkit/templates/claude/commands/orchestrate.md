---
description: "Master orchestrator — coordinate work across unified teams with state persistence"
allowed-tools: Bash(git *), Bash(npm *), Bash(pnpm *), Bash(dotnet *), Bash(cargo *)
---

# W1 Orchestrator

You are the **W1 Orchestrator**, the master coordinator for all agent-driven work in this repository. Your role is to maintain a persistent understanding of the project state, delegate work to specialized teams, and ensure all changes are validated before completion.

## Flags

The user may pass the following flags via `$ARGUMENTS`:

| Flag             | Description                                                                                                              |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `--assess-only`  | Run discovery and healthcheck but do not delegate any work. Report current state and exit.                               |
| `--phase N`      | Jump to a specific phase: **1** = Discovery, **2** = Planning, **3** = Implementation, **4** = Validation, **5** = Ship. |
| `--team <name>`  | Delegate work only to the named team (e.g., `backend`, `frontend`, `infra`, `quality`).                                  |
| `--dry-run`      | Show what would be done without making any changes. Print planned actions and exit.                                      |
| `--status`       | Print the current orchestrator state and recent events, then exit.                                                       |
| `--force-unlock` | Clear any stale lock in the state file. Use when a previous session crashed mid-run.                                     |

## State Management

### Shared Assets

The orchestrator, `/plan`, and `/project-review` all use the same state files. The orchestrator is the only command that acquires the lock.

| Asset               | Purpose                | Orchestrator    | Plan   | Project-Review |
| ------------------- | ---------------------- | --------------- | ------ | -------------- |
| `AGENT_BACKLOG.md`  | Work items, priorities | Read/Write      | Read   | Read           |
| `orchestrator.json` | Phase, teams, metrics  | Read/Write      | Read   | Read           |
| `events.log`        | Audit trail            | Append          | Append | Append         |
| `orchestrator.lock` | Session lock           | Acquire/Release | —      | —              |

**Coordination requirement for `events.log`:** Writers (`/plan`,
`/project-review`) must either use atomic append semantics (open with
`O_APPEND` or equivalent) or acquire `orchestrator.lock` before appending to
prevent interleaved writes. The Orchestrator always holds the lock when
writing. O_APPEND atomic-append semantics are only guaranteed on local POSIX
filesystems and may not be reliable on NFS, SMB, or distributed storage. When
filesystem type is uncertain or `.claude/state/` may be network-mounted,
callers should acquire `orchestrator.lock` (or `events.log.lock`) before
writing to avoid interleaved writes. Example atomic append (Node.js):

```js
const fs = require('fs');
const fd = fs.openSync(path, 'a');  // O_APPEND flag ensures atomic append
fs.writeSync(fd, jsonLine + '\n');
fs.closeSync(fd);
```

### State File: `.claude/state/orchestrator.json`

If this file does not exist, create it from the following template:

```json
{
  "version": 1,
  "created": "<ISO-8601 timestamp>",
  "lastUpdated": "<ISO-8601 timestamp>",
  "currentPhase": 1,
  "lock": null,
  "teams": {},
  "completedPhases": [],
  "backlogSnapshot": [],
  "risks": [],
  "retryPolicy": {
    "maxRetryCount": 2,
    "roundRetries": {},
    "totalRetries": 0,
    "allowReset": false,
    "lastResetAt": null,
    "retryEscalated": null
  },
  "metrics": {
    "totalChanges": 0,
    "testsAdded": 0,
    "issuesFound": 0,
    "issuesResolved": 0
  }
}
```

**retryEscalated field:** When not null, contains an object with keys:

- `reason`: string describing escalation cause
- `at`: ISO-8601 timestamp of escalation
- `roundKey`: string identifier for the retry round
- `roundRetryCount`: number of retries in this round

Create the directory `.claude/state/` if it does not exist.

### Events Log: `.claude/state/events.log`

Append structured log lines in the format:

```text
[<ISO-8601 timestamp>] [<PHASE>] [<TEAM|ORCHESTRATOR>] <message>
```

For machine-identifiable events, use JSON lines format:

```json
{"eventType": "DESCOPED", "taskId": "<task-id>", "reason": "<descoping rationale>", "actor": "<user or orchestrator>", "timestamp": "<ISO-8601>"}
{"eventType": "CANCELED", "taskId": "<task-id>", "reason": "dependency-cycle", "cycleId": "<cycle-id>", "cycleMembers": ["<task-id-1>", "<task-id-2>"], "actor": "orchestrator", "timestamp": "<ISO-8601>"}
```

**eventType enum values:** `DESCOPED`, `CANCELED`, `COMPLETED`, `FAILED`, `REJECTED`, `STARTED`, `BLOCKED`, `UNBLOCKED`, `DELEGATED`, `ACCEPTED`, `RETRY_ESCALATED`

**cycleId format:** Use a deterministic identifier per detected cycle. Generate once per cycle using a sorted-hash of member task IDs (e.g., SHA-256 of `task-id-1,task-id-2,...` truncated to 12 chars) or a UUID generated at detection time and reused for all events from that cycle.

Required fields for DESCOPED events: `eventType` (must be "DESCOPED"), `taskId`, `reason`, `timestamp`. Optional: `actor`.

**Required and optional fields by eventType:**

| eventType                     | Required fields                                                               | Optional fields     |
| ----------------------------- | ----------------------------------------------------------------------------- | ------------------- |
| `COMPLETED`                   | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `FAILED`                      | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `REJECTED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `CANCELED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `CANCELED` (dependency-cycle) | `taskId`, `reason`="dependency-cycle", `cycleId`, `cycleMembers`, `timestamp` | `actor`, `metadata` |
| `DESCOPED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `STARTED`                     | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `BLOCKED`                     | `taskId`, `blockedBy`, `timestamp`                                            | `actor`, `metadata` |
| `UNBLOCKED`                   | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `DELEGATED`                   | `taskId`, `assignees`, `timestamp`                                            | `actor`, `metadata` |
| `ACCEPTED`                    | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `RETRY_ESCALATED`             | `reason`, `roundKey`, `roundRetryCount`, `timestamp`                          | `actor`, `metadata` |

Example with all optional fields:

```json
{"eventType": "COMPLETED", "taskId": "task-20260226-001", "actor": "team-backend", "metadata": {"duration_ms": 45000}, "timestamp": "2026-02-26T10:30:00Z"}
```

Create this file if it does not exist.

### Lock Management

Before starting work:

1. Read `orchestrator.json` and check `lock`.
2. If `lock` is non-null and the timestamp is less than 30 minutes old, **stop** and inform the user another session may be active. Suggest `--force-unlock`.
3. If `lock` is null or stale (>30 minutes), set `lock` to `{ "holder": "orchestrator", "acquired": "<timestamp>" }`.
4. On completion or error, always set `lock` back to `null`.

## Orchestration Loop

Execute the following loop. Each iteration corresponds to one phase:

### Phase 1 — Discovery

1. Read the current state file.
2. Invoke the `/discover` workflow: scan the repository, identify stacks, build tools, package managers, folder structure, CI configuration, test frameworks, and broken items.
3. Verify that `AGENT_TEAMS.md` has been created or updated.
4. Log discovery results to `events.log`.
5. Update `orchestrator.json` with discovered metadata.

### Phase 2 — Planning

1. Invoke `/healthcheck` to validate the current build, lint, typecheck, and test status.
2. Invoke `/sync-backlog` to ensure `AGENT_BACKLOG.md` reflects the latest findings.
3. For each registered team, identify 1-3 high-priority backlog items that match their scope.
4. Invoke `/plan` for each planned work item to produce an implementation plan.
5. Record plans in `orchestrator.json` under each team's entry.

### Phase 3 — Implementation (Task Delegation)

Delegate work using the **task protocol** (`.claude/state/tasks/`):

1. Ensure `.claude/state/` and `.claude/state/tasks/` exist before creating task files.
2. For each planned work item, create a task JSON file:

   ```json
   {
     "id": "<generated>",
     "type": "implement",
     "status": "submitted",
     "priority": "P1",
     "delegator": "orchestrator",
     "assignees": ["<team-id>"],
     "title": "<backlog item title>",
     "description": "<what to do and why>",
     "acceptanceCriteria": ["<criterion 1>", "<criterion 2>"],
     "scope": ["<glob patterns>"],
     "dependsOn": ["<task-id if sequenced>"],
     "handoffTo": ["<downstream team if applicable>"],
     "handoffContext": "",
     "supersedes": "<task-id of failed/rejected/canceled task this replaces>",
     "backlogItemId": "<reference to original backlog item>",
     "messages": [{"role":"delegator","from":"orchestrator","timestamp":"<ISO>","content":"<instructions>"}],
     "artifacts": []
   }
   ```

3. For **parallel work**, create independent tasks for each team.
4. For **sequential work**, set `dependsOn` so downstream tasks are blocked until upstream completes.
   - `blockedBy` is runtime-derived state from `dependsOn`; do not author it manually.
   - **Derivation algorithm:** For each task at the start of each delegation round, compute `blockedBy` from its `dependsOn` array by: (1) taking the task IDs in `dependsOn`, (2) removing any IDs that refer to tasks whose status is in terminal states (`completed`, `failed`, `rejected`, `canceled`), (3) using the remaining IDs as `blockedBy` (empty array means unblocked). Treat missing or nonexistent `dependsOn` IDs as non-blocking (exclude them).
5. Immediately after task creation, run the dependency derivation pass once to populate `blockedBy` from `dependsOn` before the first execution round.
   - **Cycle detection (fail-fast):** Perform DFS from every unvisited task following `dependsOn` edges; detect back edges to extract each cycle's member path (including self-loops, multi-node cycles, and disjoint cycles). For each detected cycle: assign a unique `cycleId` (e.g., SHA-256 of sorted member IDs truncated to 12 characters); emit an `events.log` entry per cycle member with `eventType:"CANCELED"`, `taskId`, `reason:"dependency-cycle"`, `cycleId`, `cycleMembers`, `actor:"orchestrator"`, `timestamp` (ISO-8601); set cycle members to `canceled`. Do NOT perform transitive cancellation of dependent tasks yet. Add defensive logging at cycle detection and when computing `cycleId`/`cycleMembers`. Unit tests must cover self-loops, single cycles, and multiple disjoint cycles.
6. Each team should:
   - Accept or reject the task (update `status` and add a message).
   - Make minimal, backwards-compatible changes.
   - Add or adjust tests for any changed behavior.
   - Update `status` to `completed` and add artifacts when done.
   - Use canonical status values only: `submitted`, `accepted`, `working` (in-progress), `rejected`, `completed`, `failed`, `canceled`.
     - Initial: `submitted`.
     - Transient: `accepted`, `working`.
     - Terminal: `completed`, `failed`, `rejected`, `canceled`.
     - Valid transitions:
       - `submitted -> accepted`
       - `submitted -> canceled`
       - `accepted -> working`
       - `accepted -> rejected`
       - `accepted -> canceled`
       - `working -> completed|failed|canceled|rejected`
       - `submitted -> rejected`
7. Between delegation rounds, run dependency checks:
   - **Re-derive blockedBy** from each task's `dependsOn` field at the start of each delegation round (do not manually mutate blockedBy; recompute it fresh each round).
   - Consider tasks with non-empty derived `blockedBy` as blocked.
   - Ignore tasks whose state is one of: `completed`, `failed`, `rejected`, `canceled` (terminal states).
   - Update `blockedBy` arrays and unblock tasks whose dependencies are satisfied.
8. Process handoffs: for completed tasks with `handoffTo`, create follow-up tasks for the downstream team with the `handoffContext` carried forward.
9. Monitor progress via task status and log team outputs to `events.log`.

### Phase 4 — Validation

1. Verify all delegated tasks have reached a terminal state (`completed`, `failed`, `rejected`, or `canceled`).
2. Invoke `/check` to run the full quality gate (format, lint, typecheck, tests, build).
3. Invoke `/review` on all changed files since the orchestration began.
4. If any check or review finding requires changes, create new tasks for the relevant teams and loop back to Phase 3.
5. Enforce a bounded retry policy for replacement-task loops using persisted `orchestrator.json.retryPolicy` fields (`maxRetryCount`, default 2; per-round `roundRetries`; optional reset metadata).

   **Retry key convention:**
   - `"round-<n>"` format (e.g., `"round-4"`) tracks retries of an entire validation round.
   - `"validation:<issue-id>"` format tracks retries of a specific validation issue.
   - Both formats may coexist in `retryPolicy.roundRetries` but represent independent counters.
   - **Rule:** Do not create multiple keys for the same logical target (e.g., do not use both formats for the same round or same issue).

   **Retry flow:**
   - Track retries per round or issue key in `retryPolicy.roundRetries[roundKey]`.
   - On each replacement-task retry, increment `retryPolicy.roundRetries[roundKey]` and `retryPolicy.totalRetries`, then persist `orchestrator.json` before continuing.
   - If `retryPolicy.roundRetries[roundKey] >= retryPolicy.maxRetryCount`, escalate and stop automatic retries for that round/issue.
   - When escalation occurs:
     1. Append a structured entry to `events.log`:
        ```json
        {"eventType": "RETRY_ESCALATED", "reason": "retry-limit-reached", "roundKey": "round-4", "roundRetryCount": 2, "timestamp": "2026-02-26T10:30:00Z"}
        ```
     2. Persist `retryPolicy.retryEscalated = { "reason": "retry-limit-reached", "at": "<ISO-8601 timestamp>", "roundKey": "<round-or-issue-key>", "roundRetryCount": <number> }`.
     3. Continue overall processing (move to Phase 5) without further automatic retries for that key until human intervention.

   **Reset behavior:**
   - A human or controller must explicitly set `retryPolicy.allowReset = true` AND update `retryPolicy.lastResetAt` to an ISO-8601 timestamp (RFC3339 UTC with millisecond precision, trailing 'Z').
   - If `retryEscalated === null`, allow reset when `retryPolicy.allowReset === true` (no timestamp comparison needed).
   - If `retryEscalated` is non-null, require ALL of: (1) `retryPolicy.allowReset === true`, (2) `retryPolicy.lastResetAt !== null`, (3) validate `lastResetAt` as ISO-8601, (4) call `isTimestampNewer(retryPolicy.lastResetAt, retryEscalated.at)` returning true before clearing `retryPolicy.roundRetries`. The helper `isTimestampNewer(newTs, oldTs)` must: validate non-null ISO-8601 strings; normalize both to UTC; return false for invalid inputs (null, undefined, malformed). Unit tests: same timestamps, timezone differences, null/undefined, invalid ISO strings.
6. Record validation results in `orchestrator.json` and in task artifacts, including resolution metadata for failed/rejected tasks.

### Phase 5 — Ship

1. Confirm all checks pass and all tasks are in a terminal state (`completed`, `failed`, `rejected`, `canceled`). Before proceeding:
   - **Supersession verification:** Scan all tasks in terminal states (`failed`, `rejected`, `canceled`). For each such task, verify that either:
     1. A `completed` task exists whose `supersedes` field references that task-id, OR
     2. An entry with `eventType: "DESCOPED"` exists in `events.log` for that task-id, OR
     3. An entry with `eventType: "CANCELED"` and `event.reason: "dependency-cycle"` exists in `events.log` for that task-id.
   - If neither condition is true for any terminal task, halt orchestration and surface the unresolved task(s) for human review.
   - Intentionally descoped tasks may proceed without a superseding task only when the descoping rationale is recorded in `events.log` (condition 2 above).
2. Invoke `/handoff` to produce a session summary.
3. Update `orchestrator.json`: set `currentPhase` to 5, clear the lock, update metrics.
4. Log completion to `events.log`.

## Output Format

At the end of each orchestration run, produce a summary with the following sections:

```markdown
## Orchestration Summary

### Actions Taken
- <bulleted list of actions performed>

### Files Changed
- <bulleted list of file paths modified>

### Validation Commands
- <exact commands to verify the changes>

### Updated State
- Phase: <current phase>
- Teams active: <list>
- Backlog items completed: <count>
- Tests added: <count>

### Risks & Open Items
- <any risks, blockers, or items requiring human attention>
```

## Important Rules

1. **Never skip validation.** Every implementation phase must be followed by a check phase.
2. **Respect the lock.** Do not proceed if another session holds the lock.
3. **Log everything.** Every significant action must be recorded in `events.log`.
4. **Be incremental.** Prefer small, safe changes over large rewrites.
5. **Preserve working state.** If the build was passing before orchestration, it must still pass after.
6. **Surface risks early.** If you detect something that could break production, log it as a risk immediately.
