---
description: "Master orchestrator — coordinate work across unified teams with state persistence"
allowed-tools: Bash(git *), Bash(npm *), Bash(pnpm *), Bash(dotnet *), Bash(cargo *)
generated_by: "{{lastAgent}}"
last_model: "{{lastModel}}"
last_updated: "{{syncDate}}"
---

# W1 Orchestrator

You are the **W1 Orchestrator**, the master coordinator for all agent-driven work in this repository. Your role is to maintain a persistent understanding of the project state, delegate work to specialized teams, and ensure all changes are validated before completion.

## Flags

The user may pass the following flags via `$ARGUMENTS`:

| Flag             | Description                                                                                                              |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `--assess-only`  | Run discovery and healthcheck but do not delegate any work. Report current state and exit.                               |
| `--phase N`      | Jump to a specific phase: **1** = Discovery, **2** = Planning, **3** = Implementation, **4** = Validation, **5** = Ship. |
| `--team <name>`  | Delegate work only to the named team (e.g., `backend`, `frontend`, `infra`, `quality`).                                  |
| `--dry-run`      | Show what would be done without making any changes. Print planned actions and exit.                                      |
| `--status`       | Print the current orchestrator state and recent events, then exit.                                                       |
| `--force-unlock` | Clear any stale lock in the state file. Use when a previous session crashed mid-run.                                     |

## State Management

### Shared Assets

The orchestrator, `/plan`, and `/project-review` all use the same state files. The orchestrator, `/plan`, and `/project-review` may each acquire `orchestrator.lock` when writing to shared state (orchestrator for full sessions; `/plan` and `/project-review` when appending to `events.log`).

| Asset               | Purpose                | Orchestrator    | Plan              | Project-Review    |
| ------------------- | ---------------------- | --------------- | ----------------- | ----------------- |
| `AGENT_BACKLOG.md`  | Work items, priorities | Read/Write      | Read              | Read              |
| `orchestrator.json` | Phase, teams, metrics  | Read/Write      | Read              | Read              |
| `events.log`        | Audit trail            | Append          | Append            | Append            |
| `orchestrator.lock` | Session lock           | Acquire/Release | Acquire when writing | Acquire when writing |

**Coordination requirement for `events.log`:** Writers (orchestrator,
`/plan`, `/project-review`) must either use atomic append semantics (open with
`O_APPEND` or equivalent) or acquire `orchestrator.lock` before appending to
prevent interleaved writes. The orchestrator holds the lock for its full
session; `/plan` and `/project-review` acquire it only when appending (when
O_APPEND is insufficient). O_APPEND atomic-append semantics are only
guaranteed on local POSIX filesystems and may not be reliable on NFS, SMB, or
distributed storage.

**Filesystem detection:** Use a simple heuristic to decide when to acquire the lock: on Unix, inspect mount type for NFS/SMB/CIFS; on Windows, treat UNC paths (e.g., `\\server\share`) as network mounts. If the mount type indicates network storage or detection is unavailable, acquire `orchestrator.lock` before writing to `events.log`. Otherwise, O_APPEND may be used on local POSIX filesystems. **Conservative default:** When in doubt, use the lock. All non-orchestrator writers may acquire `orchestrator.lock` to eliminate ambiguity.

When filesystem type is uncertain or `.claude/state/` may be network-mounted,
callers must acquire `orchestrator.lock` before writing to avoid interleaved
writes. **Contention handling:** Wait with exponential backoff and retry; use
a configurable timeout and abort after timeout. Callers must respect the lock
acquisition protocol and abort after timeout. Example atomic append (Node.js):

```js
const fs = require('fs');
const fd = fs.openSync(path, 'a');  // O_APPEND flag ensures atomic append
fs.writeSync(fd, jsonLine + '\n');
fs.closeSync(fd);
```

### State File: `.claude/state/orchestrator.json`

If this file does not exist, create it from the following template:

```json
{
  "version": 1,
  "created": "<ISO-8601 timestamp>",
  "lastUpdated": "<ISO-8601 timestamp>",
  "currentPhase": 1,
  "lock": null,
  "teams": {},
  "completedPhases": [],
  "backlogSnapshot": [],
  "risks": [],
  "retryPolicy": {
    "maxRetryCount": 2,
    "roundRetries": {},
    "totalRetries": 0,
    "allowReset": false,
    "lastResetAt": null,
    "retryEscalated": null
  },
  "metrics": {
    "totalChanges": 0,
    "testsAdded": 0,
    "issuesFound": 0,
    "issuesResolved": 0
  }
}
```

**retryEscalated field:** When not null, contains an object with keys:

- `reason`: string describing escalation cause
- `at`: ISO-8601 timestamp of escalation
- `roundKey`: string identifier for the retry round
- `roundRetryCount`: number of retries in this round

Create the directory `.claude/state/` if it does not exist.

### Events Log: `.claude/state/events.log`

Append structured log lines in the format:

```text
[<ISO-8601 timestamp>] [<PHASE>] [<TEAM|ORCHESTRATOR>] <message>
```

For machine-identifiable events, use JSON lines format:

```json
{"eventType": "DESCOPED", "taskId": "<task-id>", "reason": "<descoping rationale>", "actor": "<user or orchestrator>", "timestamp": "<ISO-8601>"}
{"eventType": "CANCELED", "taskId": "<task-id>", "reason": "dependency-cycle", "cycleId": "<cycle-id>", "cycleMembers": ["<task-id-1>", "<task-id-2>"], "actor": "orchestrator", "timestamp": "<ISO-8601>"}
```

**eventType enum values:** `DESCOPED`, `CANCELED`, `COMPLETED`, `FAILED`, `REJECTED`, `STARTED`, `BLOCKED`, `UNBLOCKED`, `DELEGATED`, `ACCEPTED`, `RETRY_ESCALATED`

**cycleId format:** Generate a deterministic identifier per detected cycle. Hash the sorted list of member task IDs with SHA-256 and truncate to 12 chars. Reuse the same cycleId for all events from that cycle.

Required fields for DESCOPED events: `eventType` (must be "DESCOPED"), `taskId`, `reason`, `timestamp`. Optional: `actor`.

**Required and optional fields by eventType:**

| eventType                     | Required fields                                                               | Optional fields     |
| ----------------------------- | ----------------------------------------------------------------------------- | ------------------- |
| `COMPLETED`                   | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `FAILED`                      | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `REJECTED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `CANCELED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `CANCELED` (dependency-cycle) | `taskId`, `reason`="dependency-cycle", `cycleId`, `cycleMembers`, `timestamp` | `actor`, `metadata` |
| `DESCOPED`                    | `taskId`, `reason`, `timestamp`                                               | `actor`, `metadata` |
| `STARTED`                     | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `BLOCKED`                     | `taskId`, `blockedBy`, `timestamp`                                            | `actor`, `metadata` |
| `BLOCKED` (blocked-on-canceled) | `taskId`, `blockedBy`, `blockedReason`="canceled", `timestamp`                 | `actor`, `metadata` |
| `UNBLOCKED`                   | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `DELEGATED`                   | `taskId`, `assignees`, `timestamp`                                            | `actor`, `metadata` |
| `ACCEPTED`                    | `taskId`, `timestamp`                                                         | `actor`, `metadata` |
| `RETRY_ESCALATED`             | `reason`, `roundKey`, `roundRetryCount`, `timestamp`                          | `actor`, `metadata` |

Example with all optional fields:

```json
{"eventType": "COMPLETED", "taskId": "task-20260226-001", "actor": "team-backend", "metadata": {"duration_ms": 45000}, "timestamp": "2026-02-26T10:30:00Z"}
```

Create this file if it does not exist.

### Lock Management

Before starting work:

1. Read `orchestrator.json` and check `lock`.
2. If `lock` is non-null and the timestamp is less than 30 minutes old, **stop** and inform the user another session may be active. Suggest `--force-unlock`.
3. If `lock` is null or stale (>30 minutes), set `lock` to `{ "holder": "orchestrator", "acquired": "<timestamp>" }`.
4. On completion or error, always set `lock` back to `null`.

## Orchestration Loop

Execute the following loop. Each iteration corresponds to one phase:

### Phase 1 — Discovery

1. Read the current state file.
2. Invoke the `/discover` workflow: scan the repository, identify stacks, build tools, package managers, folder structure, CI configuration, test frameworks, and broken items.
3. Verify that `AGENT_TEAMS.md` has been created or updated.
4. Log discovery results to `events.log`.
5. Update `orchestrator.json` with discovered metadata.

### Phase 2 — Planning

1. Invoke `/healthcheck` to validate the current build, lint, typecheck, and test status.
2. Invoke `/sync-backlog` to ensure `AGENT_BACKLOG.md` reflects the latest findings.
3. For each registered team, identify 1-3 high-priority backlog items that match their scope.
4. Invoke `/plan` for each planned work item to produce an implementation plan.
5. Record plans in `orchestrator.json` under each team's entry.

### Phase 3 — Implementation (Task Delegation)

Delegate work using the **task protocol** (`.claude/state/tasks/`):

1. Ensure `.claude/state/` and `.claude/state/tasks/` exist before creating task files.
2. For each planned work item, create a task JSON file:

   ```json
   {
     "id": "<generated>",
     "type": "implement",
     "status": "submitted",
     "priority": "P1",
     "delegator": "orchestrator",
     "assignees": ["<team-id>"],
     "title": "<backlog item title>",
     "description": "<what to do and why>",
     "acceptanceCriteria": ["<criterion 1>", "<criterion 2>"],
     "scope": ["<glob patterns>"],
     "dependsOn": ["<task-id if sequenced>"],
     "handoffTo": ["<downstream team if applicable>"],
     "handoffContext": "",
     "supersedes": "<task-id of failed/rejected/canceled task this replaces>",
     "backlogItemId": "<reference to original backlog item>",
     "messages": [{"role":"delegator","from":"orchestrator","timestamp":"<ISO>","content":"<instructions>"}],
     "artifacts": []
   }
   ```

3. For **parallel work**, create independent tasks for each team.
4. For **sequential work**, set `dependsOn` so downstream tasks are blocked until upstream completes.
   - `blockedBy` is runtime-derived state from `dependsOn`; do not author it manually.
   - **Cycle detection (run once at start):** Before deriving `blockedBy`, run a single DFS-based detection pass over the graph formed by `task.dependsOn`. Perform DFS from every unvisited task following `dependsOn` edges; detect back edges to extract each cycle's member path (including self-loops, multi-node cycles, and disjoint cycles). If any cycle is found: (1) assign a unique `cycleId` (e.g., SHA-256 of sorted member IDs truncated to 12 characters); (2) emit an `events.log` entry per cycle member with `eventType:"CANCELED"`, `taskId`, `reason:"dependency-cycle"`, `cycleId`, `cycleMembers`, `actor:"orchestrator"`, `timestamp` (ISO-8601); (3) set cycle members' status to `canceled`; (4) abort the derivation pass and surface a clear error. Do not proceed with `blockedBy` derivation until cycles are resolved. References: `dependsOn`, `optionalDependsOn`, `blockedBy`, `strictDependsOnValidation`.
   - **Derivation algorithm:** For each task at the start of each delegation round, compute `blockedBy` from its `dependsOn` array by: (1) taking the task IDs in `dependsOn` (excluding any listed in the task's `optionalDependsOn`), (2) for IDs that refer to tasks in terminal states: if status is `completed`, remove from `blockedBy`; if status is `failed`, `rejected`, or `canceled`, **do NOT remove** — keep the ID in `blockedBy` and record `blockedReason: "canceled"` (or set downstream status to `BLOCKED_ON_CANCELED` when all blockers are canceled), (3) for IDs that refer to in-progress tasks, keep in `blockedBy`. Empty `blockedBy` means unblocked. **Option 2 (no transitive cancellation):** This document uses Option 2. Downstream tasks that depend on cycle-canceled tasks remain explicitly blocked until manual descoping or retry. Do NOT perform transitive cancellation. **Validation pass:** For each `dependsOn` ID that does not match any known task ID: if `strictDependsOnValidation` is true (default), treat missing IDs as **fatal** — halt delegation and surface an error; if false, emit a warning and treat as non-blocking. IDs in `optionalDependsOn` may be missing without fatal error; still log missing references. Unit tests must cover: self-loops, single cycles, multiple disjoint cycles, and downstream tasks (verify `blockedBy` contains canceled IDs and downstream status is `BLOCKED_ON_CANCELED`).
5. Immediately after task creation, run cycle detection once, then the dependency derivation pass once to populate `blockedBy` from `dependsOn` before the first execution round.
6. Each team should:
   - Accept or reject the task (update `status` and add a message).
   - Make minimal, backwards-compatible changes.
   - Add or adjust tests for any changed behavior.
   - Update `status` to `completed` and add artifacts when done.
   - Use canonical status values only: `submitted`, `accepted`, `working` (in-progress), `rejected`, `completed`, `failed`, `canceled`, `BLOCKED_ON_CANCELED`.
     - Initial: `submitted`.
     - Transient: `accepted`, `working`.
     - Terminal: `completed`, `failed`, `rejected`, `canceled`, `BLOCKED_ON_CANCELED`.
     - `BLOCKED_ON_CANCELED`: Set when a task's `blockedBy` contains only canceled (or failed/rejected) dependencies; remains until manual descoping or retry.
     - Valid transitions:
       - `submitted -> accepted`
       - `submitted -> canceled`
       - `accepted -> working`
       - `accepted -> rejected`
       - `accepted -> canceled`
       - `working -> completed|failed|canceled|rejected`
       - `submitted -> rejected`
7. Between delegation rounds, run dependency checks:
   - **Re-derive blockedBy** from each task's `dependsOn` field at the start of each delegation round (do not manually mutate blockedBy; recompute it fresh each round). Do NOT remove canceled dependencies from `blockedBy`; keep their IDs and set `blockedReason: "canceled"` or status `BLOCKED_ON_CANCELED` when all blockers are canceled.
   - Consider tasks with non-empty derived `blockedBy` as blocked.
   - Ignore tasks whose state is one of: `completed`, `failed`, `rejected`, `canceled`, `BLOCKED_ON_CANCELED` (terminal states).
   - Update `blockedBy` arrays and unblock tasks whose dependencies are satisfied (completed only; never unblock when blocked only by canceled deps).
8. Process handoffs: for completed tasks with `handoffTo`, create follow-up tasks for the downstream team with the `handoffContext` carried forward.
9. Monitor progress via task status and log team outputs to `events.log`.

### Phase 4 — Validation

1. Verify all delegated tasks have reached a terminal state (`completed`, `failed`, `rejected`, or `canceled`).
2. Invoke `/check` to run the full quality gate (format, lint, typecheck, tests, build).
3. Invoke `/review` on all changed files since the orchestration began.
4. If any check or review finding requires changes, create new tasks for the relevant teams and loop back to Phase 3.
5. Enforce a bounded retry policy for replacement-task loops using persisted `orchestrator.json.retryPolicy` fields (`maxRetryCount`, default 2; per-round `roundRetries`; optional reset metadata).

   **Retry key convention:**
   - `"round-<n>"` format (e.g., `"round-4"`) tracks retries of an entire validation round.
   - `"validation:<issue-id>"` format tracks retries of a specific validation issue.
   - Both formats may coexist in `retryPolicy.roundRetries` but represent independent counters.
   - **Rule:** Do not create multiple keys for the same logical target (e.g., do not use both formats for the same round or same issue).

   **Retry flow:**
   - Track retries per round or issue key in `retryPolicy.roundRetries[roundKey]`.
   - On each replacement-task retry, increment `retryPolicy.roundRetries[roundKey]` and `retryPolicy.totalRetries`, then persist `orchestrator.json` before continuing.
   - If `retryPolicy.roundRetries[roundKey] >= retryPolicy.maxRetryCount`, escalate and stop automatic retries for that round/issue.
   - When escalation occurs:
     1. Append a structured entry to `events.log`:

        ```json
        {"eventType": "RETRY_ESCALATED", "reason": "retry-limit-reached", "roundKey": "round-4", "roundRetryCount": 2, "timestamp": "2026-02-26T10:30:00Z"}
        ```

     2. Persist `retryPolicy.retryEscalated = { "reason": "retry-limit-reached", "at": "<ISO-8601 timestamp>", "roundKey": "<round-or-issue-key>", "roundRetryCount": <number> }`.
     3. Continue overall processing (move to Phase 5) without further automatic retries for that key until human intervention.

   **Reset behavior:** Implement `shouldResetRetryState` and `isTimestampNewer` as exported utilities in the orchestrator's validation module (e.g., `orchestrator/validation.ts`). Use them to enforce the exact rules:
   - **`isTimestampNewer(newTs?: string | null, oldTs?: string | null): boolean`** — Validate non-null ISO-8601 strings; normalize both to UTC; return false for null, undefined, malformed inputs.
   - **`shouldResetRetryState(retryPolicy, retryEscalated): Promise<{allowed: boolean, reason?: string}>`** — Require `retryPolicy.allowReset === true`. If `retryEscalated === null`, allow reset when `allowReset === true` (no timestamp comparison needed). If `retryEscalated` is non-null, require `retryPolicy.lastResetAt` to be non-null and a valid ISO-8601 timestamp; call `isTimestampNewer(retryPolicy.lastResetAt, retryEscalated.at)` and only allow clearing `retryPolicy.roundRetries` when it returns true. When `isTimestampNewer` returns false: do NOT clear `roundRetries`; return `{allowed: false, reason: "reset prevented: lastResetAt not newer than retryEscalated.at"}` and ensure calling code logs/surfaces this reason.
   - Unit tests: same timestamps, timezone differences, null/undefined, invalid ISO strings, and the negative case where isTimestampNewer returns false (roundRetries remains unchanged).
6. Record validation results in `orchestrator.json` and in task artifacts, including resolution metadata for failed/rejected tasks.

### Phase 5 — Ship

1. Confirm all checks pass and all tasks are in a terminal state (`completed`, `failed`, `rejected`, `canceled`). Before proceeding:
   - **Supersession verification:** Scan all tasks in terminal states (`failed`, `rejected`, `canceled`). For each such task, verify that either:
     1. A `completed` task exists whose `supersedes` field references that task-id, OR
     2. An entry with `eventType: "DESCOPED"` exists in `events.log` for that task-id, OR
     3. An entry with `eventType: "CANCELED"` exists in `events.log` for that task-id (any reason, including `dependency-cycle`, user/manual, or non-replacement cancellation).
   - If neither condition is true for any terminal task, halt orchestration and surface the unresolved task(s) for human review.
   - **DESCOPED vs CANCELED:** Emit DESCOPED for intentional removal that needs rationale recorded; emit CANCELED for user/manual or non-replacement cancellation.
2. Invoke `/handoff` to produce a session summary.
3. Update `orchestrator.json`: set `currentPhase` to 5, clear the lock, update metrics.
4. Log completion to `events.log`.

## Output Format

At the end of each orchestration run, produce a summary with the following sections:

```markdown
## Orchestration Summary

### Actions Taken
- <bulleted list of actions performed>

### Files Changed
- <bulleted list of file paths modified>

### Validation Commands
- <exact commands to verify the changes>

### Updated State
- Phase: <current phase>
- Teams active: <list>
- Backlog items completed: <count>
- Tests added: <count>

### Risks & Open Items
- <any risks, blockers, or items requiring human attention>
```

## Important Rules

1. **Never skip validation.** Every implementation phase must be followed by a check phase.
2. **Respect the lock.** Do not proceed if another session holds the lock.
3. **Log everything.** Every significant action must be recorded in `events.log`.
4. **Be incremental.** Prefer small, safe changes over large rewrites.
5. **Preserve working state.** If the build was passing before orchestration, it must still pass after.
6. **Surface risks early.** If you detect something that could break production, log it as a risk immediately.
