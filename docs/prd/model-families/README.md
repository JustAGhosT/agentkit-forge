# Model Family Benchmark Dossiers

These files capture benchmark evidence by model family so team model guides
can use a shared evidence base.

## Source Quality Tags

- -Fetched-: Data was retrieved directly from a source URL.
- -Search snippet-: Data comes from search result summaries only.
- -Vendor claim-: Data is from a model provider page, not independent.
- -Independent benchmark-: Data is from benchmark operators/research sites.

## Dossiers

- [Anthropic Claude](./model-family-claude.md)
- [OpenAI GPT](./model-family-openai.md)
- [Google Gemini](./model-family-gemini.md)
- [Qwen](./model-family-qwen.md)
- [MiniMax](./model-family-minimax.md)
- [xAI Grok](./model-family-grok.md)
- [GLM](./model-family-glm.md)
- [Kimi](./model-family-kimi.md)
- [DeepSeek](./model-family-deepseek.md)
- [Mistral (Codestral)](./model-family-mistral.md)
- [Meta Llama](./model-family-llama.md)
- [Cohere (Command)](./model-family-cohere.md)
- [Amazon Nova](./model-family-amazon-nova.md)
- [IBM Granite](./model-family-ibm-granite.md)
- [Cursor Composer](./model-family-cursor-composer.md)
- [Windsurf SWE](./model-family-windsurf-swe.md)

## How to Use With Team Model Guides

1. Pull benchmark facts from these dossiers into each team guide's
   -Data Provenance and Refresh- section.
2. Only promote a model into ranked tiers after at least one
   -Independent benchmark- point plus one runtime metric.
3. Keep uncertain entries in -Newly Tracked Models- until validated.
4. Re-run weighting calculations when cost or benchmark deltas are material.
